{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distillation Example: Faster Speech Transcription through Model Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model distillation** is a machine learning technique where knowledge from a large, complex model (often referred to as the \"teacher\" model) is transferred to a smaller, simpler model (known as the \"student\" model). The goal is to make the student model mimic the teacher's performance, thereby achieving high accuracy while being more efficient in terms of computational resources and speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Our task in the following is to transcribe recorded speech into text. We want to choose a model that gives us an optimal tradeoff of transcription speed and quality. We compare the **whisper** model and its distilled variant, **distil-whisper**._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai_dojo import show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this demo, we use the [**LibriSpeech Corpus**](https://paperswithcode.com/dataset/librispeech), a collection of approximately 1,000 hours of audiobooks that are a part of the LibriVox project. Most of the audiobooks come from the Project Gutenberg. In this dataset, the audiobook narrations are already split up into chapters and short segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"../data/audio/tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "librispeech_dataset = torchaudio.datasets.LIBRISPEECH(\n",
    "    data_dir,\n",
    "    url=\"dev-clean\",\n",
    "    download=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at an example audio segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform, sample_rate, _, _, _, _ = librispeech_dataset[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play the audio\n",
    "show.audio(waveform, sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we process the dataset into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def create_audio_dataframe(dataset, base_path, num_samples=5):\n",
    "    \"\"\"\n",
    "    Create a DataFrame containing audio players and metadata for a given audio dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset (torchaudio.dataset): The audio dataset from which to load samples.\n",
    "        base_path (str): The base directory where the dataset files are stored.\n",
    "        num_samples (int): Number of samples to include in the DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with an 'Audio Player', file path, and metadata for each sample.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for i in range(min(num_samples, len(dataset))):  # Process only available samples\n",
    "        waveform, sample_rate, transcript, speaker_id, chapter_id, utterance_id = dataset[i]\n",
    "        \n",
    "        # Convert utterance_id to string and apply zero-padding\n",
    "        utterance_id_str = str(utterance_id).zfill(4)\n",
    "        file_name = f\"{speaker_id}-{chapter_id}-{utterance_id_str}.flac\"\n",
    "        file_path = f\"{base_path}/{speaker_id}/{chapter_id}/{file_name}\"\n",
    "\n",
    "        audio_player = show.audio(waveform, sample_rate)\n",
    "        data.append({\n",
    "            \"Sample ID\": i,\n",
    "            \"Audio Player\": audio_player,\n",
    "            \"File Path\": file_path,\n",
    "            #\"Sample Rate\": sample_rate,\n",
    "            \"Transcript\": transcript,\n",
    "            \"Speaker ID\": speaker_id,\n",
    "            \"Chapter ID\": chapter_id,\n",
    "            \"Utterance ID\": utterance_id\n",
    "        })\n",
    "        \n",
    "\n",
    "    data = pd.DataFrame(data)\n",
    "    data = data.set_index([\"Speaker ID\", \"Chapter ID\", \"Utterance ID\"])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To limit transcription time, we take only a few rows from the top. Adjust this number if you want a bigger or smaller benchmark set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 32\n",
    "n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with audio players\n",
    "audio_df = create_audio_dataframe(\n",
    "    librispeech_dataset,\n",
    "    num_samples=n_samples,\n",
    "    base_path=f\"{data_dir}/LibriSpeech/dev-clean/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_dataframe_with_audio(df):\n",
    "    from IPython.display import display\n",
    "    # Render DataFrame with audio players using HTML representation\n",
    "    display(df.style.format({'Audio Player': lambda x: x._repr_html_()}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display_dataframe_with_audio(audio_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcription with the Original Whisper Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply the original **whisper** model as published by OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show.github_repo(\"https://github.com/openai/whisper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}  # collection of transcription models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a first demonstration, we use the smallest available model variant. There is a tradeoff between model size and transcript quality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"whisper\"\n",
    "model_variant = \"tiny.en\"\n",
    "\n",
    "original_model_name = f\"{model_type} {model_variant}\"\n",
    "\n",
    "# Load a Whisper model\n",
    "models[original_model_name] = whisper.load_model(model_variant)\n",
    "models[original_model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to transcribe the audio with different models from different sources. Here is a function for that that aims to be generic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(model, file_path):\n",
    "    \"\"\"\n",
    "    Transcribe audio using the provided model (either Whisper or Hugging Face pipeline) given the file path.\n",
    "    \n",
    "    Args:\n",
    "        model: Loaded Whisper model or Hugging Face ASR pipeline.\n",
    "        file_path (str): Path to the audio file.\n",
    "    \n",
    "    Returns:\n",
    "        str: The transcribed text.\n",
    "    \"\"\"\n",
    "    # Check if the model is a Whisper model by checking for the 'transcribe' attribute\n",
    "    if hasattr(model, 'transcribe'):\n",
    "        result = model.transcribe(file_path)\n",
    "        return result['text']\n",
    "    # If it's not a Whisper model, assume it's a Hugging Face pipeline\n",
    "    else:\n",
    "        result = model(file_path)\n",
    "        # Extract the transcription text from the pipeline output, which is typically a list of dictionaries\n",
    "        return result['text'] if result else \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we want to transcribe all the audio in the given dataframe. Here is a function that does this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "def transcribe_data(df, model, output_col=\"Transcription\"):\n",
    "    \"\"\"\n",
    "    Adds transcriptions to the DataFrame containing audio file paths and measures the time taken to complete the transcription.\n",
    "    Displays a progress bar and suppresses all warnings during the process.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the 'File Path' column.\n",
    "        model: Loaded Whisper model or Hugging Face ASR pipeline.\n",
    "        output_col (str): Name of the column to store the transcription results. Defaults to \"Transcription\".\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame with a new 'Transcription' column.\n",
    "        float: Total time taken for the transcription process in seconds.\n",
    "    \"\"\"\n",
    "    start_time = time.time()  # Start timing\n",
    "    \n",
    "    # Suppress warnings\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        \n",
    "        # Apply transcription to each file path in the DataFrame with a progress bar\n",
    "        tqdm.pandas(desc=\"Transcribing Audio Files\")\n",
    "        df[output_col] = df['File Path'].progress_apply(lambda x: transcribe_audio(model, x))\n",
    "    \n",
    "    end_time = time.time()  # End timing\n",
    "    total_time = end_time - start_time  # Calculate total time taken\n",
    "    \n",
    "    return df, total_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we test the function on a single audio segment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = transcribe_audio(models[original_model_name], audio_df.iloc[0][\"File Path\"])\n",
    "show.text(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to start the transcription job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription_time = {} # record the time to transcribe in [s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_col = f\"Transcription {original_model_name}\"\n",
    "audio_data_transcribed, transcription_time[original_model_name] = transcribe_data(\n",
    "    audio_df,\n",
    "    models[original_model_name],\n",
    "    output_col=output_col,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and here is the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_dataframe_with_audio(\n",
    "    audio_data_transcribed.head(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Evaluation\n",
    "\n",
    "The [**Word Error Rate (WER)**](https://en.wikipedia.org/wiki/Word_error_rate) is a suitable metric for the transcription quality. The simple formula for WER is:\n",
    "\n",
    "\n",
    "$$\\text{WER} = \\frac{\\text{S} + \\text{D} + \\text{I}}{\\text{N}}$$\n",
    "\n",
    "where:\n",
    "- **S** is the number of substitutions (words that are incorrectly recognized),\n",
    "- **D** is the number of deletions (words that are missed),\n",
    "- **I** is the number of insertions (extra words that are added),\n",
    "- **N** is the total number of words in the reference transcription.\n",
    "\n",
    "WER is expressed as a percentage, representing the proportion of words that were incorrectly recognized. A lower WER indicates better performance of the speech recognition system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from jiwer import wer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess the text by removing punctuation and converting to lowercase.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text string.\n",
    "    \n",
    "    Returns:\n",
    "        str: Preprocessed text string.\n",
    "    \"\"\"\n",
    "    # Remove punctuation using regex\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Convert text to lowercase to ensure case insensitivity\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def calculate_wer(ground_truth, transcription):\n",
    "    \"\"\"\n",
    "    Calculate the Word Error Rate (WER) between ground truth and transcription.\n",
    "    \n",
    "    Args:\n",
    "        ground_truth (str): The correct text.\n",
    "        transcription (str): The output text from the speech recognition system.\n",
    "    \n",
    "    Returns:\n",
    "        float: The Word Error Rate expressed as a percentage.\n",
    "    \"\"\"\n",
    "    # Preprocess both ground truth and transcription\n",
    "    ground_truth = preprocess_text(ground_truth)\n",
    "    transcription = preprocess_text(transcription)\n",
    "    \n",
    "    # Calculate WER using jiwer library\n",
    "    error_rate = wer(ground_truth, transcription)\n",
    "    return error_rate * 100  # Convert to percentage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function adds the WER as a column by comparing two transcript columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def compute_error_rate(df, ground_truth_col, transcription_col, error_rate_col='WER'):\n",
    "    \"\"\"\n",
    "    Compute the Word Error Rate (WER) for each row in a DataFrame and add it as a new column.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the transcripts.\n",
    "        ground_truth_col (str): Column name for the ground truth transcripts.\n",
    "        transcription_col (str): Column name for the machine-generated transcripts.\n",
    "        error_rate_col (str): Column name for the resulting error rate. Defaults to 'WER'.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The original DataFrame with an additional column for the error rate.\n",
    "    \"\"\"\n",
    "    df[error_rate_col] = df.apply(\n",
    "        lambda row: calculate_wer(row[ground_truth_col], row[transcription_col]),\n",
    "        axis=1\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_data_transcribed = compute_error_rate(\n",
    "    audio_data_transcribed,\n",
    "    ground_truth_col=\"Transcript\",\n",
    "    transcription_col=output_col,\n",
    "    error_rate_col=f\"WER {original_model_name}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_dataframe_with_audio(\n",
    "    audio_data_transcribed.head(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was a pretty quick transcription process using the \"tiny\" variant of _whisper_. Are you happy with the results? There may be room for improvement by using the larger model variants. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Bigger Model Variant for Better Transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We move up two steps and choose the \"medium\" size variant of whisper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai_dojo.plot import model_size_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage:\n",
    "model_sizes = [(\"whisper tiny.en\", 39e6), (\"whispher medium.en\", 769e6)]\n",
    "model_size_comparison(model_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_variant = \"medium.en\"\n",
    "bigger_model_name = f\"{model_type} {model_variant}\"\n",
    "bigger_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[bigger_model_name] = whisper.load_model(model_variant)\n",
    "models[bigger_model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now repeat the transcription with the larger model. This is probably going to take a few minutes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_col = f\"Transcription {bigger_model_name}\"\n",
    "audio_data_transcribed, transcription_time[bigger_model_name] = transcribe_data(\n",
    "    audio_data_transcribed,\n",
    "    models[bigger_model_name],\n",
    "    output_col=output_col,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_data_transcribed = compute_error_rate(\n",
    "    audio_data_transcribed,\n",
    "    ground_truth_col=\"Transcript\",\n",
    "    transcription_col=output_col,\n",
    "    error_rate_col=f\"WER {bigger_model_name}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_dataframe_with_audio(\n",
    "    audio_data_transcribed.head(10),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compare_error_rates(data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Calculates the average of each column starting with \"WER\" in the DataFrame and plots a horizontal bar chart using pandas' built-in plotting capabilities.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): A DataFrame containing columns with names starting with \"WER\".\n",
    "    \"\"\"\n",
    "    # Filter columns that start with 'WER'\n",
    "    wer_columns = [col for col in data.columns if col.startswith('WER')]\n",
    "    \n",
    "    # Calculate the mean of these columns\n",
    "    wer_means = data[wer_columns].mean()\n",
    "    \n",
    "    # Plotting using pandas' built-in plot method for a horizontal bar chart\n",
    "    ax = wer_means.plot.barh(figsize=(8, 1), title='Average Word Error Rate (WER) Comparison')\n",
    "    \n",
    "    # Labeling axes\n",
    "    ax.set_xlabel('Average WER (%)')\n",
    "    ax.set_ylabel('Models')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "def compare_transcription_time(transcription_time):\n",
    "     ax = (\n",
    "        pd.Series(transcription_time.values(), index=transcription_time.keys())\n",
    "        .plot(kind=\"barh\", figsize=(8, 1))\n",
    "    )\n",
    "     ax.set_xlabel('Transcription Time [s]')\n",
    "     ax.set_ylabel('Models')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_error_rates(audio_data_transcribed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_transcription_time(transcription_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have significantly improved transcript quality, but at the cost of a multiple of the compute time. Can we get both, high transcript quality and speed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter Distil-Whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**distil-whisper** is a distilled version of whisper for the English language. The distilled variants are 50% smaller, and the authors claim that the model transcribes up to 6 times faster with a very small quality loss. Let's put it to the test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show.github_repo(\"https://github.com/huggingface/distil-whisper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_sizes += [(\"distil-whispher medium.en\", 394e6)]\n",
    "model_size_comparison(model_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain _distil-whisper_ from Huggingface hub as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_id = \"distil-whisper/distil-small.en\"\n",
    "\n",
    "# Load the model with the specified device and dtype settings\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_safetensors=True,\n",
    ")\n",
    "\n",
    "# Load the processor\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline for automatic speech recognition with the specified settings\n",
    "model_type = \"distil-whisper\"\n",
    "model_variant = \"medium.en\"\n",
    "distilled_model_name = f\"{model_type} {model_variant}\"\n",
    "\n",
    "models[distilled_model_name] = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    #max_new_tokens=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[distilled_model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now rerun the transcription job with a distilled version of the bigger, higher quality model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_col = f\"Transcription {distilled_model_name}\"\n",
    "audio_data_transcribed, transcription_time[distilled_model_name] = transcribe_data(\n",
    "    audio_df,\n",
    "    models[distilled_model_name],\n",
    "    output_col=output_col,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_data_transcribed = compute_error_rate(\n",
    "    audio_data_transcribed,\n",
    "    ground_truth_col=\"Transcript\",\n",
    "    transcription_col=output_col,\n",
    "    error_rate_col=f\"WER {distilled_model_name}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_dataframe_with_audio(\n",
    "    audio_data_transcribed.head(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_error_rates(audio_data_transcribed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_transcription_time(transcription_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speedup = transcription_time[\"whisper medium.en\"] / transcription_time[\"distil-whisper medium.en\"] \n",
    "print(f\"Speedup factor achieved: {round(speedup, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Our quick experiment generally confirms what the authors of _distil-whisper_ have reported: The distilled version indeed offers significant speedup at the cost of only a very small transcription quality loss. In many use cases, the distilled model will be preferable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "_This notebook is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/). Copyright © 2024 [Christian Staudt](https://clstaudt.me), [Katharina Rasch](https://krasch.io)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-on-a-microbudget",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
