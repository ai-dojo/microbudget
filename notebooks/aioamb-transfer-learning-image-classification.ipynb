{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning Example: Adapting a Pre-Trained Convolutional Neural Network for Custom Image Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning involves taking a model that has been trained on one task and applying it to a different but related task. This technique is useful because it allows a model to leverage knowledge (features, weights, and biases) from the original task, which can often improve learning efficiency and prediction performance on the new task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Our task in the following is to reuse a neural network that has been trained on a different image classification problem to solve another one.**_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Clasification Task & Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use [**Oxford Flowers102**](https://paperswithcode.com/dataset/oxford-102-flower) dataset, which contains images of 102 different types of flowers. The size of the dataset is about 700 MB.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets\n",
    "\n",
    "dataset, dataset_info = tensorflow_datasets.load(\n",
    "    \"oxford_flowers102\",\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    "    split=[\"train\", \"test\", \"validation\"],\n",
    ")\n",
    "\n",
    "# Unpack the dataset\n",
    "train_data, test_data, validation_data = dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look into the images and their class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 102\n",
    "\n",
    "# Assuming 'class_names' is a list of flower names corresponding to label indices\n",
    "class_names = dataset_info.features['label'].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def display_images_with_labels(dataset, class_names, num_samples=5):\n",
    "    \"\"\"\n",
    "    Display a grid of images and their labels.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset: tf.data.Dataset - The dataset from which to take the samples.\n",
    "    - class_names: list - A list of class names corresponding to the label indices.\n",
    "    - num_samples: int - The number of samples to display.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 2 * num_samples))  # Adjust figure size based on the number of samples\n",
    "\n",
    "    # Take 'num_samples' samples from the dataset\n",
    "    for i, (image, label) in enumerate(dataset.take(num_samples)):\n",
    "        ax = plt.subplot(1, num_samples, i + 1)  # Create a subplot for each sample\n",
    "        ax.imshow(image.numpy().astype(\"uint8\"))  # Convert to uint8 type if necessary\n",
    "        ax.set_title(f\"Label: {class_names[label.numpy()]}\")\n",
    "        ax.axis(\"off\")  # Hide the axes\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout so images are neatly displayed\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display_images_with_labels(train_data, class_names, num_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Transfer Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick internet search did not give us a pre-trained model for this particular flower classification problem. So we need to train our own. But do we need to train it from scratch? Training an image classifier, often a convolutional neural network, may take significant compute time. \n",
    "\n",
    "Can we do more with less? Yes, we can reuse most parts of a network that has already been trained on a large image dataset, and only retrain the final layer(s) for our specific task. This approach is a classic example of transfer learning.\n",
    "\n",
    "Transfer learning works well for image classification problems because the early layers of a CNN learn to detect generic features like edges, textures, and patterns, which are relevant across various image domains. The later layers learn more specific features tailored to the task at hand. By leveraging a pre-trained model, we can benefit from the knowledge it has already acquired and adapt it to our specific problem with minimal retraining.\n",
    "\n",
    "This allows us to achieve good performance with less data and computation, as we only need to train the last layer(s) instead of training the entire network from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"img/Visualization-of-Transfer-Learning-4.jpg\" alt=\"Visualization of Transfer Learning\" style=\"max-width:100%; height:auto;\"/>\n",
    "    <br>\n",
    "    <figcaption style=\"color: gray;\">Source: <a href=\"https://www.researchgate.net/figure/Visualization-of-Transfer-Learning-4_fig5_331983081\">A Study Review: Semantic segmentation with Deep Neural Networks</a></figcaption>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting a Pretrained Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_MobileNetV2_ is a good choice as a base model for transfer learning because it is lightweight, computationally efficient, and has been pre-trained on a large dataset.\n",
    "\n",
    "It has been pre-trained on the ImageNet dataset, which contains over 1.4 million images across 1,000 classes. This diverse and large-scale dataset allows MobileNetV2 to learn a wide range of features that can be effectively transferred to other image classification tasks.\n",
    "\n",
    "Its compact architecture makes it suitable for mobile and resource-constrained environments while still achieving good performance. By leveraging MobileNetV2's pre-learned features, we can quickly adapt it to our specific task with minimal re-training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"img/The-visual-structure-of-MobileNetV2-18860668.png\" style=\"max-width:100%; height:auto;\"/>\n",
    "    <br>\n",
    "    <figcaption style=\"color: gray;\">MobileNetV2 architecture visualized (<a href=\"https://www.researchgate.net/figure/The-visual-structure-of-MobileNetV2_fig3_356354780\">Source</a>)</figcaption>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of `keras` we obtain the pre-trained model. Note the following arguments:\n",
    "\n",
    "- `input_shape=(224, 224, 3)`: Specifies the expected input shape of the images. Here, the images will be resized to 224x224 pixels with 3 color channels (RGB).\n",
    "- `include_top=False`: Excludes the fully connected layer at the top of the network. This allows us to customize the final layer(s) for our specific classification task.\n",
    "- `weights=\"imagenet\"`: Initializes the model with pre-trained weights from the ImageNet dataset, enabling transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MobileNetV2 without the top layer\n",
    "base_model = keras.applications.MobileNetV2(\n",
    "    input_shape=(224, 224, 3),\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    ")\n",
    "\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Dataset for Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code prepares the image data for training, validation, and testing:\n",
    "\n",
    "- The preprocessing function resizes images and normalizes pixel values.\n",
    "- The labels are converted to one-hot encoded format, since the network architecture expects this.\n",
    "- The training, validation, and test data pipelines apply preprocessing, shuffle the training data, batch the data, and use prefetching to optimize data loading.\n",
    "- These steps ensure the data is in the correct format and efficiently loaded during model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image, label):\n",
    "    # Resize and normalize the images\n",
    "    image = tf.image.resize(image, [224, 224])\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    label = tf.one_hot(label, depth=n_classes)  # Convert labels to one-hot\n",
    "    return image, label\n",
    "\n",
    "\n",
    "# Apply preprocessing, shuffle, batch, and prefetch to optimize loading\n",
    "train_data = (\n",
    "    train_data\n",
    "    .map(preprocess)\n",
    "    .shuffle(1000)\n",
    "    .batch(32)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "validation_data = (\n",
    "    validation_data\n",
    "    .map(preprocess)\n",
    "    .batch(32)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "test_data = (\n",
    "    test_data\n",
    "    .map(preprocess)\n",
    "    .batch(32)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Assuming `train_data` is your dataset\n",
    "for images, labels in train_data.take(1):\n",
    "    print(\"Label examples:\", labels.numpy())\n",
    "    print(\"Shape of labels:\", labels.shape)\n",
    "    print(\"Batch shape:\", images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble Model for Transfer Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now assemble the new model to be trained on flower classification. \n",
    "\n",
    "First, we \"freeze\" the weights in the base model - the convolutional part of the network - so they are unaffected by training. We want to reuse them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False  # Freeze the convolutional base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our network needs a new classification head. The original model has an output layer for 1000 classes  by default - change it to the number of flower types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model on top of the output of the base model\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(n_classes, activation='softmax')  # Output layer n_classes\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we compile the model to set it up for training. As performance metrics, we look at _accuracy_, _precision_ and _recall_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\", Precision(), Recall()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model. We can expect the training to be much faster since only the weights of the classification head need to be adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on the training data\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    epochs=n_epochs,  # You can adjust the number of epochs based on the training performance\n",
    "    validation_data=validation_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(history):\n",
    "    # Convert the history.history dict to a pandas DataFrame\n",
    "    history_df = pd.DataFrame(history.history)\n",
    "    \n",
    "    # Use the 'plot' method to plot the DataFrame, one column vs. the index\n",
    "    history_df.plot(figsize=(10, 5))\n",
    "    plt.grid(True)\n",
    "    #plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "    plt.title('Training and Validation Loss and Accuracy')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well our new model does in terms of metrics..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy, test_precision, test_recall = model.evaluate(test_data)\n",
    "\n",
    "# Calculate F1 Score\n",
    "if test_precision + test_recall > 0:\n",
    "    test_f1_score = 2 * (test_precision * test_recall) / (test_precision + test_recall)\n",
    "else:\n",
    "    test_f1_score = None  # Handle case where F1 score cannot be calculated\n",
    "\n",
    "# Create a DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    \"Metric\": [\"Loss\", \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"],\n",
    "    \"Value\": [\n",
    "        test_loss,\n",
    "        test_accuracy,\n",
    "        test_precision,\n",
    "        test_recall,\n",
    "        test_f1_score\n",
    "    ]\n",
    "})\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and let's have a look at a sample of the image set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "def display_sample_predictions(model, dataset, class_names):\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Take a single batch from the dataset\n",
    "    for images, labels in dataset.shuffle(100).take(1):\n",
    "        predictions = model.predict(images)\n",
    "        \n",
    "        # Reverse the normalization, assuming the images were normalized to [0, 1]\n",
    "        images = images * 255\n",
    "        \n",
    "        for i in range(9):  # Display the first 9 images and predictions\n",
    "            ax = plt.subplot(3, 3, i + 1)\n",
    "            # Clip the values to be in the range [0, 255] and convert to integers\n",
    "            img = tf.clip_by_value(images[i], 0, 255).numpy().astype(\"uint8\")\n",
    "            \n",
    "            plt.imshow(img)\n",
    "            predicted_label = class_names[np.argmax(predictions[i])]\n",
    "            \n",
    "            # Check if labels are one-hot encoded or integers\n",
    "            if labels.ndim == 2:  # One-hot encoded\n",
    "                true_label_index = np.argmax(labels[i])\n",
    "            else:  # Integer labels\n",
    "                true_label_index = labels[i]\n",
    "                \n",
    "            true_label = class_names[true_label_index]\n",
    "            \n",
    "            plt.title(f\"Predicted: {predicted_label},\\n True: {true_label}\")\n",
    "            plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'class_names' is a list of flower names corresponding to label indices\n",
    "class_names = dataset_info.features['label'].names\n",
    "\n",
    "# Display predictions\n",
    "display_sample_predictions(model, test_data, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This simple workflow is a demonstration of how we can obtain a pretty good image classifier for a new classification tasks with minimal training resources. Neural networks are mostly feature extractors, so learning makes it possible to reuse that knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Model for Later Use\n",
    "\n",
    "We are going to reuse this model. This is how to save it to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "model_dir = Path(\"../models/tmp\")\n",
    "\n",
    "# Assuming 'model' is your Keras model\n",
    "model.save(model_dir / \"flowers-classifier-tl.keras\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "_This notebook is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/). Copyright Â© 2024 [Christian Staudt](https://clstaudt.me), [Katharina Rasch](https://krasch.io)_\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-on-a-microbudget",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
