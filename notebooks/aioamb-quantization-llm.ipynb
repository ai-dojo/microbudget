{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization Example: Running a Large Language Model with Different Levels of Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we want to compare the capabilities of an LLM in different quantized versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, LLMs are often large - in terms of file size and memory footprint. A conventient way of managing local LLMs is Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai_dojo import show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show.github_repo(\"https://github.com/ollama/ollama\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the installation steps lined out in the ollama repo to install it and start it on your system. Alternatively, if you have docker, you can just do\n",
    "\n",
    "```\n",
    "docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use this Python module to interact with the ollama system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show.github_repo(\"https://github.com/ollama/ollama-python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A LLM in Quantized Versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> Rocket ðŸ¦ is a 3 billion large language model that was trained on a mix of publicly available datasets. [...] The outcome is a highly effective chat model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantized Versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to get some quantized versions of this model. A ðŸ¤— user has already applied various quantization methods to the model and provided the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_user = \"TheBloke\"\n",
    "model_name = \"rocket-3B-GGUF\"\n",
    "full_model_name = f\"{huggingface_user}/{model_name}\"\n",
    "model_page_url= f\"https://huggingface.co/{full_model_name}\"\n",
    "model_page_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**â„¹ The GGUF File Format**\n",
    "\n",
    "[**GGUF**](https://huggingface.co/docs/hub/gguf) is a binary format that is optimized for quick loading and saving of models, making it highly efficient for inference purposes. Unlike tensor-only file formats like safetensors, GGUF encodes both the tensors and a standardized set of metadata\n",
    "\n",
    "\n",
    "![](https://cdn-lfs.huggingface.co/datasets/huggingface/documentation-images/60dc8f9e25311d5ab671019499edd6f847bf3c9796d97b5579240c652ef445da?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27gguf-spec.png%3B+filename%3D%22gguf-spec.png%22%3B&response-content-type=image%2Fpng&Expires=1715686360&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNTY4NjM2MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9kYXRhc2V0cy9odWdnaW5nZmFjZS9kb2N1bWVudGF0aW9uLWltYWdlcy82MGRjOGY5ZTI1MzExZDVhYjY3MTAxOTQ5OWVkZDZmODQ3YmYzYzk3OTZkOTdiNTU3OTI0MGM2NTJlZjQ0NWRhP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiZyZXNwb25zZS1jb250ZW50LXR5cGU9KiJ9XX0_&Signature=hiPC3Nn7JtVghKOC-CCE5imxed5dmote3JYSSQkdHA-hM7bvvFoJ-QtGYYzxaDaiV6ldIzhjDkLO1FwWK3OC6fx3ocKxRvfh0ebB-l8qwGxOCuGVePaj7b2a9Z6i54EkGwD6n7lUX6A4i0Ip%7ExlAAJsj7I7NzWLfjtE7O-XZE-Lq-iQsqOyaAR26tYTAhD4PpaicGVfXt-Kxko8MzKxE2zrHs4vpXodjPsRKxSJWuytjWMmXNH1-PptvCV35Y1HM1UpbytyX8nhyGg3lchYFvQa6keI%7Ehhh-o05755%7EUin2dLXhZpdjcqtqpkM0BJytBspU4onjdIrlH-Ay4PUzaIQ__&Key-Pair-Id=KVTP0A1DKRTAX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model page lists a variety of quantized models in different quantization types. Let's focus on a few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_types = [\n",
    "    \"Q8_0\",\n",
    "    \"Q4_K_M\",\n",
    "    \"Q2_K\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These quantized models are not available from the Ollama model registry. Fortunately, adding custom models downloaded from ðŸ¤— to Ollama is quite straightforward. We need to give Ollama a model name and a simple configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model_paths = {}\n",
    "for qtype in tqdm.tqdm(quantization_types):\n",
    "    model_file = f\"rocket-3b.{qtype}.gguf\" \n",
    "    tqdm.tqdm.write(f\"Downloading {qtype} model\")\n",
    "    model_path = huggingface_hub.hf_hub_download(full_model_name, filename=model_file)\n",
    "    quantized_model_paths[qtype] = model_path\n",
    "    \n",
    "quantized_model_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_file(model_path) -> str:\n",
    "    \"\"\"Creates a simple Ollama model configuration file for the given serialized model.\"\"\"\n",
    "    content = f\"FROM {model_path}\"\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for qtype, model_path in quantized_model_paths.items():\n",
    "    ollama_model_name = f\"{model_name}:{qtype}\"\n",
    "    print(f\"Creating Ollama model {ollama_model_name}\")\n",
    "    response = ollama.create(\n",
    "        model=ollama_model_name,\n",
    "        modelfile=make_model_file(model_path)\n",
    "    )\n",
    "    print(response[\"status\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that all of the models above are now available in Ollama. Also, have a look at the size on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from pandas import DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_ollama_model_table() -> DataFrame:\n",
    "    \"\"\"List Ollama models in tabular form\"\"\"\n",
    "    model_data = pandas.json_normalize(ollama.list()[\"models\"])\n",
    "    model_data = model_data[[\"model\", \"size\", \"details.family\", \"details.format\", \"details.parameter_size\", \"details.quantization_level\"]]\n",
    "    model_data[\"size\"] = model_data[\"size\"].apply(lambda s: round(s / (1024**3), 2))\n",
    "    return model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_ollama_model_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "Let's prompt the original and the quantized models to do something useful. This should give us a first impression of how model performance relates to the level of quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt = \"Recommend some sights in Florence, Italy.\"\n",
    "show.text(test_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8-bit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_level = \"Q8_0\"\n",
    "ollama_model_name = f\"{model_name}:{quantization_level}\"\n",
    "ollama_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat(\n",
    "    model=ollama_model_name,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": test_prompt,\n",
    "        },\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "show.stream(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_level = \"Q4_K_M\"\n",
    "ollama_model_name = f\"{model_name}:{quantization_level}\"\n",
    "ollama_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat(\n",
    "    model=ollama_model_name,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": test_prompt,\n",
    "        },\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "show.stream(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_level = \"Q2_K\"\n",
    "ollama_model_name = f\"{model_name}:{quantization_level}\"\n",
    "ollama_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat(\n",
    "    model=ollama_model_name,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": test_prompt,\n",
    "        },\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "show.stream(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you rate the quality of each response? Letting a large, highly factual LLM review them might help you spot the errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "_This notebook is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/). Copyright Â© 2024 [Christian Staudt](https://clstaudt.me), [Katharina Rasch](https://krasch.io)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
